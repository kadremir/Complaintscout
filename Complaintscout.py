import cloudscraper
from bs4 import BeautifulSoup
import csv
import time
import requests
import re

# KullanÄ±cÄ±dan kelime ve sayfa sayÄ±sÄ± al
anahtar_kelime = input("Hangi kelimeyi iÃ§eren ÅŸikayetler Ã§ekilsin? ").lower().strip()
sayfa_sayisi = int(input("KaÃ§ sayfa taransÄ±n? "))

scraper = cloudscraper.create_scraper()

def analiz_yap(metin):
    """Ollama API'si kullanarak metin analizi yapar"""
    prompt = f"""AÅŸaÄŸÄ±daki ÅŸikayet metnini analiz ederek TÃ¼rkÃ§e olarak:
1. Duygu Durumu: (Olumlu, Olumsuz, NÃ¶tr, Ã–fke, Memnuniyet vb.)
2. Anahtar Kelimeler: (virgÃ¼lle ayrÄ±lmÄ±ÅŸ)
3. Ã‡Ã¶zÃ¼m Ã–nerisi: (kÄ±sa ve Ã¶z)

Metin: {metin}

LÃ¼tfen cevabÄ± tam olarak ÅŸu formatta ver:
Duygu Durumu: ...
Anahtar Kelimeler: ...
Ã‡Ã¶zÃ¼m Ã–nerisi: ..."""

    try:
        response = requests.post(
            '[YOUR-API-URL(OLLAMA SERVE)]',
            json={
                'model': 'mistral',
                'prompt': prompt,
                'stream': False,
                'options': {'temperature': 0.3}
            }
        )
        
        if response.status_code == 200:
            return response.json()['response']
        return None
        
    except Exception as e:
        print(f"Analiz hatasÄ±: {e}")
        return None

def analiz_ayir(metin):
    """Model Ã§Ä±ktÄ±sÄ±nÄ± ayrÄ±ÅŸtÄ±rma"""
    duygu = re.search(r"Duygu Durumu:\s*(.+?)\n", metin)
    anahtar = re.search(r"Anahtar Kelimeler:\s*(.+?)\n", metin)
    cozum = re.search(r"Ã‡Ã¶zÃ¼m Ã–nerisi:\s*(.+?)(\n|$)", metin)
    
    return (
        duygu.group(1).strip() if duygu else "Belirlenemedi",
        anahtar.group(1).strip() if anahtar else "Yok",
        cozum.group(1).strip() if cozum else "Yok"
    )

with open("sikayetler.csv", mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(["BaÅŸlÄ±k", "Ä°Ã§erik", "Duygu Durumu", "Anahtar Kelimeler", "Ã‡Ã¶zÃ¼m Ã–nerisi"])

    for sayfa in range(1, sayfa_sayisi + 1):
        print(f"ğŸ“„ Sayfa {sayfa} iÅŸleniyor...")
        url = f"https://www.sikayetvar.com/sikayetler?page={sayfa}"
        response = scraper.get(url)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            titles = soup.find_all(class_='complaint-title')

            for title in titles:
                baÅŸlÄ±k = title.get_text(strip=True)
                link_tag = title.find('a')

                if anahtar_kelime in baÅŸlÄ±k.lower() and link_tag and link_tag['href']:
                    detay_link = "https://www.sikayetvar.com" + link_tag['href']
                    detay_response = scraper.get(detay_link)

                    if detay_response.status_code == 200:
                        detay_soup = BeautifulSoup(detay_response.text, 'html.parser')
                        detay = detay_soup.find(class_='complaint-detail-description')
                        iÃ§erik = detay.get_text(strip=True) if detay else "Ä°Ã§erik bulunamadÄ±."

                        # Yapay zeka analizi
                        analiz_sonucu = analiz_yap(iÃ§erik)
                        
                        if analiz_sonucu:
                            duygu, anahtar, cozum = analiz_ayir(analiz_sonucu)
                            writer.writerow([baÅŸlÄ±k, iÃ§erik, duygu, anahtar, cozum])
                            print(f"âœ… Analiz edildi: {baÅŸlÄ±k}")
                        else:
                            writer.writerow([baÅŸlÄ±k, iÃ§erik, "Hata", "Hata", "Hata"])
                        
                        time.sleep(1.5)  # Ollama iÃ§in bekleme sÃ¼resi
        else:
            print(f"âŒ Sayfa alÄ±namadÄ±: {url}")

        time.sleep(2.5)

print("\nâœ… Analizler tamamlandÄ±! sikayetler.csv dosyasÄ±nÄ± kontrol edin.")